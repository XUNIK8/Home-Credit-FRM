{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running --> Home Credit Default Risk - Pytorch Neural Network\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "print(\"Running --> Home Credit Default Risk - Pytorch Neural Network\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"GPU NAME --> \", torch.cuda.get_device_name(0))\n",
    "\n",
    "# SET HYPERPARAMETERS\n",
    "hp_test_size = 0.2\n",
    "hp_epochs = 12\n",
    "hr_batch_size = 320\n",
    "hp_lr= 0.000008\n",
    "hp_emb_drop = 0.04\n",
    "hp_layers = [800, 350]\n",
    "hp_ps = [0.001,0.01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD DATA\n",
    "application_train_df = pd.read_csv('../input/application_train.csv').sample(frac = 1)\n",
    "application_test_df = pd.read_csv('../input/application_test.csv')\n",
    "previous_application_df = pd.read_csv('../input/previous_application.csv')\n",
    "\n",
    "application_train_df['CSV_SOURCE'] = 'application_train.csv'\n",
    "application_test_df['CSV_SOURCE'] = 'application_test.csv'\n",
    "df = pd.concat([application_train_df, application_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_492/2165703345.py:95: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df['house_variables_sum'] = df[house_vars].sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MANAGE previous_applications.csv\n",
    "temp_previous_df = previous_application_df.groupby('SK_ID_CURR', as_index=False).agg({'NAME_CONTRACT_STATUS': lambda x: ','.join(set(','.join(x).split(',')))})\n",
    "temp_previous_df['has_only_approved'] = np.where(temp_previous_df['NAME_CONTRACT_STATUS'] == 'Approved', '1', '0')\n",
    "temp_previous_df['has_been_rejected'] = np.where(temp_previous_df['NAME_CONTRACT_STATUS'].str.contains('Refused'), '1', '0')\n",
    "\n",
    "# JOIN DATA\n",
    "df = pd.merge(df, temp_previous_df, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# CREATE CUSTOM COLUMNS\n",
    "#################################################### total_amt_req_credit_bureau\n",
    "df['total_amt_req_credit_bureau'] = (\n",
    "  df['AMT_REQ_CREDIT_BUREAU_YEAR'] * 1 + \n",
    "  df['AMT_REQ_CREDIT_BUREAU_QRT'] * 2 + \n",
    "  df['AMT_REQ_CREDIT_BUREAU_MON'] * 8 + \n",
    "  df['AMT_REQ_CREDIT_BUREAU_WEEK'] * 16 + \n",
    "  df['AMT_REQ_CREDIT_BUREAU_DAY'] * 32 +\n",
    "  df['AMT_REQ_CREDIT_BUREAU_HOUR'] * 64)\n",
    "df['total_amt_req_credit_bureau_isnull'] = np.where(df['total_amt_req_credit_bureau'].isnull(), '1', '0')\n",
    "df['total_amt_req_credit_bureau'].fillna(0, inplace=True)\n",
    "\n",
    "#######################################################################  has_job\n",
    "df['has_job'] = np.where(df['NAME_INCOME_TYPE'].isin(['Pensioner', 'Student', 'Unemployed']), '1', '0')\n",
    "\n",
    "#######################################################################  has_children\n",
    "df['has_children'] = np.where(df['CNT_CHILDREN'] > 0, '1', '0')\n",
    "\n",
    "####################################################### clusterise_days_employed\n",
    "def clusterise_days_employed(x):\n",
    "    days = x['DAYS_EMPLOYED']\n",
    "    if days > 0:\n",
    "      return 'not available'\n",
    "    else:\n",
    "      days = abs(days)\n",
    "      if days < 30:\n",
    "        return 'less 1 month'\n",
    "      elif days < 180:\n",
    "        return 'less 6 months'\n",
    "      elif days < 365:\n",
    "        return 'less 1 year'\n",
    "      elif days < 1095:\n",
    "        return 'less 3 years'\n",
    "      elif days < 1825:\n",
    "        return 'less 5 years'\n",
    "      elif days < 3600:\n",
    "        return 'less 10 years'\n",
    "      elif days < 7200:\n",
    "        return 'less 20 years'\n",
    "      elif days >= 7200:\n",
    "        return 'more 20 years'\n",
    "      else:\n",
    "        return 'not available'\n",
    "df['cluster_days_employed'] = df.apply(clusterise_days_employed, axis=1)\n",
    "\n",
    "#######################################################################  custom_ext_source_3\n",
    "def clusterise_ext_source(x):\n",
    "    if str(x) == 'nan':\n",
    "      return 'not available'\n",
    "    else:\n",
    "      if x < 0.1:\n",
    "        return 'less 0.1'\n",
    "      elif x < 0.2:\n",
    "        return 'less 0.2'\n",
    "      elif x < 0.3:\n",
    "        return 'less 0.3'\n",
    "      elif x < 0.4:\n",
    "        return 'less 0.4'\n",
    "      elif x < 0.5:\n",
    "        return 'less 0.5'\n",
    "      elif x < 0.6:\n",
    "        return 'less 0.6'\n",
    "      elif x < 0.7:\n",
    "        return 'less 0.7'\n",
    "      elif x < 0.8:\n",
    "        return 'less 0.8'\n",
    "      elif x < 0.9:\n",
    "        return 'less 0.9'\n",
    "      elif x <= 1:\n",
    "        return 'less 1'\n",
    "df['clusterise_ext_source_1'] = df['EXT_SOURCE_1'].apply(lambda x: clusterise_ext_source(x))\n",
    "df['clusterise_ext_source_2'] = df['EXT_SOURCE_2'].apply(lambda x: clusterise_ext_source(x))\n",
    "df['clusterise_ext_source_3'] = df['EXT_SOURCE_3'].apply(lambda x: clusterise_ext_source(x))\n",
    "\n",
    "#######################################################################  house_variables_sum\n",
    "house_vars = ['APARTMENTS_AVG','APARTMENTS_MEDI','APARTMENTS_MODE','BASEMENTAREA_AVG',\n",
    "  'BASEMENTAREA_MEDI','BASEMENTAREA_MODE','COMMONAREA_AVG','COMMONAREA_MEDI',\n",
    "  'COMMONAREA_MODE','ELEVATORS_AVG','ELEVATORS_MEDI','ELEVATORS_MODE','EMERGENCYSTATE_MODE',\n",
    "  'ENTRANCES_AVG','ENTRANCES_MEDI','ENTRANCES_MODE','FLOORSMAX_AVG','FLOORSMAX_MEDI',\n",
    "  'FLOORSMAX_MODE','FLOORSMIN_AVG','FLOORSMIN_MEDI','FLOORSMIN_MODE','FONDKAPREMONT_MODE',\n",
    "  'HOUSETYPE_MODE','LANDAREA_AVG','LANDAREA_MEDI','LANDAREA_MODE','LIVINGAPARTMENTS_AVG',\n",
    "  'LIVINGAPARTMENTS_MEDI','LIVINGAPARTMENTS_MODE','LIVINGAREA_AVG','LIVINGAREA_MEDI','LIVINGAREA_MODE',\n",
    "  'NONLIVINGAPARTMENTS_AVG','NONLIVINGAPARTMENTS_MEDI','NONLIVINGAPARTMENTS_MODE','NONLIVINGAREA_AVG',\n",
    "  'NONLIVINGAREA_MEDI','NONLIVINGAREA_MODE','TOTALAREA_MODE','WALLSMATERIAL_MODE',\n",
    "  'YEARS_BEGINEXPLUATATION_AVG','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BEGINEXPLUATATION_MODE',\n",
    "  'YEARS_BUILD_AVG','YEARS_BUILD_MEDI','YEARS_BUILD_MODE']\n",
    "df['house_variables_sum'] = df[house_vars].sum(axis=1)\n",
    "df['house_variables_sum_isnull'] = np.where(df['house_variables_sum'].isnull(), '1', '0')\n",
    "df['house_variables_sum'].fillna(value=df['house_variables_sum'].median(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT COLUMNS\n",
    "numerical_columns = [\n",
    "  'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n",
    "  'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION',\n",
    "  'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'DAYS_EMPLOYED', 'DAYS_LAST_PHONE_CHANGE',\n",
    "  'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'total_amt_req_credit_bureau',\n",
    "  'house_variables_sum']\n",
    "categorical_columns = [\n",
    "  'CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'NAME_EDUCATION_TYPE', 'FLAG_OWN_REALTY', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE',\n",
    "  'NAME_CONTRACT_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE',\n",
    "  'has_only_approved', 'has_been_rejected', 'has_job', 'has_children', 'cluster_days_employed',\n",
    "  'clusterise_ext_source_1', 'clusterise_ext_source_2', 'clusterise_ext_source_3',\n",
    "  'total_amt_req_credit_bureau_isnull', 'house_variables_sum_isnull']\n",
    "\n",
    "target_column = ['TARGET']\n",
    "df = df[numerical_columns + categorical_columns + target_column]\n",
    "\n",
    "# MANAGE MISSING VALUES\n",
    "for numerical_column in numerical_columns:\n",
    "  if df[numerical_column].isnull().values.any():\n",
    "    df[numerical_column + '_isnull'] = np.where(df[numerical_column].isnull(), '1', '0')\n",
    "  df[numerical_column].fillna(value=df[numerical_column].median(), inplace=True)\n",
    "\n",
    "for categorical_column in categorical_columns:\n",
    "  df[categorical_column].fillna('NULL', inplace=True)\n",
    "\n",
    "# STANDARDISE\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df[numerical_columns] = pd.DataFrame(min_max_scaler.fit_transform(df[numerical_columns]))\n",
    "\n",
    "# CONVERT CATEGORICAL COLUMNS INTO TYPE \"category\"\n",
    "categorical_columns.remove('CSV_SOURCE')\n",
    "\n",
    "for column in categorical_columns:\n",
    "  df[column] = LabelEncoder().fit_transform(df[column].astype(str))\n",
    "  df[column] = df[column].astype('category')\n",
    "    \n",
    "# SPLIT DATA INTO TRAINING vs TRAIN\n",
    "train_df = df[df['CSV_SOURCE'] == 'application_train.csv']\n",
    "train_output_df = pd.DataFrame(train_df['TARGET'], columns=['TARGET'])\n",
    "\n",
    "test_df = df[df['CSV_SOURCE'] == 'application_test.csv']\n",
    "\n",
    "# REMOVE NOT USEFUL COLUMNS\n",
    "train_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)\n",
    "test_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)\n",
    "\n",
    "# CREATE VALIDATION TEST\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(train_df, train_output_df, test_size=hp_test_size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING TENSORS...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CREATE TENSORS\n",
    "print(\"CREATING TENSORS...\")\n",
    "def create_tensors(input_df):\n",
    "  stack = []\n",
    "  for column in input_df.columns:\n",
    "    if input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
    "      stack.append(input_df[column].astype(np.float64))\n",
    "    else:\n",
    "      stack.append(input_df[column].cat.codes.values)\n",
    "  return torch.tensor(np.stack(stack, 1), dtype=torch.float)\n",
    "\n",
    "tensor_x_train_cat = create_tensors(x_train[categorical_columns]).float().to(device)\n",
    "tensor_x_train_num = create_tensors(x_train[numerical_columns]).float().to(device)\n",
    "tensor_y_train = torch.tensor(y_train.values).flatten().float().to(device)\n",
    "\n",
    "tensor_x_valid_cat = create_tensors(x_validation[categorical_columns]).float().to(device)\n",
    "tensor_x_valid_num = create_tensors(x_validation[numerical_columns]).float().to(device)\n",
    "tensor_y_valid = torch.tensor(y_validation.values).flatten().float().to(device)\n",
    "\n",
    "tensor_x_test_cat = create_tensors(test_df[categorical_columns]).float().to(device)\n",
    "tensor_x_test_num = create_tensors(test_df[numerical_columns]).float().to(device)\n",
    "\n",
    "# CREATE CATEGORICAL EMBEDDING SIZES\n",
    "categorical_columns_size = [len(df[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_columns_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(input_df):\n",
    "    stack = []\n",
    "    for column in input_df.columns:\n",
    "        if input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
    "            stack.append(input_df[column].astype(np.float64))\n",
    "        else:\n",
    "            stack.append(input_df[column].cat.codes.values)\n",
    "    return torch.tensor(np.stack(stack, 1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_train_cat = create_tensors(x_train[categorical_columns]).float().to(device)\n",
    "tensor_x_train_num = create_tensors(x_train[numerical_columns]).float().to(device)\n",
    "tensor_y_train = torch.tensor(y_train.values).flatten().float().to(device)\n",
    "\n",
    "tensor_x_valid_cat = create_tensors(x_validation[categorical_columns]).float().to(device)\n",
    "tensor_x_valid_num = create_tensors(x_validation[numerical_columns]).float().to(device)\n",
    "tensor_y_valid = torch.tensor(y_validation.values).flatten().float().to(device)\n",
    "\n",
    "tensor_x_test_cat = create_tensors(test_df[categorical_columns]).float().to(device)\n",
    "tensor_x_test_num = create_tensors(test_df[numerical_columns]).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? Why create categorical_embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_size = [len(df[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_columns_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating Model\n",
      "Training Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (all_embeddings): ModuleList(\n",
       "    (0): Embedding(3, 2)\n",
       "    (1): Embedding(2, 1)\n",
       "    (2): Embedding(5, 3)\n",
       "    (3): Embedding(2, 1)\n",
       "    (4): Embedding(19, 10)\n",
       "    (5): Embedding(58, 29)\n",
       "    (6): Embedding(2, 1)\n",
       "    (7): Embedding(6, 3)\n",
       "    (8): Embedding(6, 3)\n",
       "    (9): Embedding(8, 4)\n",
       "    (10): Embedding(8, 4)\n",
       "    (11): Embedding(3, 2)\n",
       "    (12): Embedding(3, 2)\n",
       "    (13): Embedding(2, 1)\n",
       "    (14): Embedding(2, 1)\n",
       "    (15): Embedding(9, 5)\n",
       "    (16): Embedding(11, 6)\n",
       "    (17): Embedding(10, 5)\n",
       "    (18): Embedding(10, 5)\n",
       "    (19): Embedding(2, 1)\n",
       "    (20): Embedding(1, 1)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.04, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=107, out_features=800, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.001, inplace=False)\n",
       "    (4): Linear(in_features=800, out_features=350, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(350, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.01, inplace=False)\n",
       "    (8): Linear(in_features=350, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Neural Network Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, input_size, num_numerical_cols, layers, ps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.emb_drop = nn.Dropout(hp_emb_drop)\n",
    "        self.bn_cont = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        layerlist = []\n",
    "        for i, elem in enumerate(layers):\n",
    "            layerlist.append(nn.Linear(input_size, elem))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(layers[i]))\n",
    "            layerlist.append(nn.Dropout(ps[i]))\n",
    "            input_size = elem\n",
    "        layerlist.append(nn.Linear(layers[-1], 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "    \n",
    "    def forward(self, x_c, x_n):\n",
    "        embeddings = [e(x_c[:, i].long()) for i , e in enumerate(self.all_embeddings)]\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x_n = self.bn_cont(x_n)\n",
    "        x = torch.cat([x, x_n], 1)\n",
    "        x = self.layers(x)\n",
    "        return x \n",
    "\n",
    "print(\"Instantiating Model\")\n",
    "num_numerical_cols = tensor_x_train_num.shape[1]\n",
    "num_categorical_cols = sum((nf for ni, nf in categorical_embedding_sizes))\n",
    "initial_input_size = num_categorical_cols + num_numerical_cols\n",
    "model = Model(categorical_embedding_sizes, initial_input_size, num_numerical_cols, \\\n",
    "    layers=hp_layers, ps=hp_ps)\n",
    "sigmoid = nn.Sigmoid() # for converting output to probability\n",
    "loss_function = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp_lr) # Adam Optimizer\n",
    "model.to(device) # send model to GPU\n",
    "\n",
    "# Train Neural Network Model\n",
    "print(\"Training Model...\")\n",
    "train_tensor_dataset = TensorDataset(tensor_x_train_cat, tensor_x_train_num, tensor_y_train)\n",
    "train_loader = DataLoader(dataset=train_tensor_dataset, batch_size=hr_batch_size, shuffle=True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 0\tloss: 0.6825921385123919\tauc: 0.6348876650351257\n",
      "\tepoch: 1\tloss: 0.638388938882106\tauc: 0.6859974748858926\n",
      "\tepoch: 2\tloss: 0.6066870703343762\tauc: 0.7247301436486915\n",
      "\tepoch: 3\tloss: 0.5504036545753479\tauc: 0.7264288440990685\n",
      "\tepoch: 4\tloss: 0.48060575581030973\tauc: 0.7333068373089993\n",
      "\tepoch: 5\tloss: 0.4136315893057264\tauc: 0.7370069938077342\n",
      "\tepoch: 6\tloss: 0.35559813308157756\tauc: 0.7370956848666356\n",
      "\tepoch: 7\tloss: 0.3095558968957596\tauc: 0.744481397811536\n",
      "\tepoch: 8\tloss: 0.27657297995760166\tauc: 0.7558961150438839\n",
      "\tepoch: 9\tloss: 0.2551725139776039\tauc: 0.7646401604420894\n",
      "\tepoch: 10\tloss: 0.24443458646574628\tauc: 0.7709217641604327\n",
      "\tepoch: 11\tloss: 0.2414347133338839\tauc: 0.7767443071082971\n"
     ]
    }
   ],
   "source": [
    "tot_y_train_in = []\n",
    "tot_y_train_out = []\n",
    "\n",
    "for epoch in range(hp_epochs):\n",
    "    train_losses = []\n",
    "    for x_cat, x_num, y in train_loader:\n",
    "        y_train = model(x_cat, x_num)\n",
    "        single_loss = loss_function(sigmoid(y_train.squeeze()), y)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(single_loss.item())\n",
    "        tot_y_train_in.append(y)\n",
    "        tot_y_train_out.append(y_train)\n",
    "    epoch_loss = 1.0 * sum(train_losses) / len(train_losses)\n",
    "    epoch_auc = roc_auc_score(torch.cat(tot_y_train_in).cpu().numpy(), \\\n",
    "        torch.cat(tot_y_train_out).cpu().detach().numpy())\n",
    "    tot_y_train_in = []\n",
    "    tot_y_train_out = []\n",
    "    print(\"\\tepoch: \" + str(epoch) + \"\\tloss: \" + str(epoch_loss) + \"\\tauc: \" + str(epoch_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb5b8ce5f619f9a0dbadd29041b3333b5b190cf93f0eef1091f3c1650edd19a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
